\documentclass{../tudscript}
\author{Jeroen Trzaska}
\title{Mathe VL 19}

\begin{document}
\sect{Hypergeometrischeverteilung}
\begin{itemize}
\item X hat die Wertemenge ${\{0,1,\cdots, n\}}$ und die Werteverteilung
  $\begin{pmatrix}
    0 & 1 & 2& \cdots &n \\
    p_0 & p_1 & p_2 & \cdots &p_n
  \end{pmatrix}
$
  mit \ilmath{\sum_{i=0}^n p_1 = 1 \text{ und }  p(x=i) = p_i = \frac{\binom{M}{i} *
      \binom{N-M}{n-i}}{\binom{N}{n}}} für $i= 0,1,\cdots,n$
\item $E(x) = n * \frac{M}{N}= n * p$, da $p_i = \frac{M}{N}$
\item $D^2(x)= n * \frac{M}{N} (1 - \frac{M}{N}) \frac{N-n}{N-1}$
 
\end{itemize}
\ilmath{\overbrace{\text{Hypergeometrischeverteilung} \underbrace{\rightarrow}_{N \rightarrow \infty, M  \rightarrow \infty} \text{Binomialverteilung}}^{\text{endlich viele Werte}} \overbrace{\underbrace{\rightarrow}_{n \rightarrow
  \infty,p \rightarrow \infty} \text{Poisonverteilung}}^{\text{abzählbar unendlich viele Werte }}}

\sect{Geometrischeverteilung}
\begin{itemize}
\item X hat die Wertemenge $\{1,2,\cdots\}$ und die Werteverteilung
  $
  \begin{pmatrix}
    1 &2& \cdots \\
    p_1 & p_2 & \cdots
\end{pmatrix}
$
mit $p(x = i) = p_i = (1-p)^{i-1}p$ für $i = 1, 2, \cdots$
Insbesonder gilt:
\ilmath{\sum_{i=1}^{\infty} p_i = \sum_{i=1}^{\infty} p(1-p)^{i-1} = p \sum_{i=1}^{\infty}(1-p)^k = p \frac{1}{1-(1-p)} = p * \frac{1}{p} = 1}
\item X gibt die Anzahl der Bernoulli-Versuche bis zum ersten Erfolg an
\end{itemize}
\ssect{Ableitung der Summenformel der geometrischen Reihe $(0 \leq q \leq 1)$ :}
\ilmath{&\sum_{i=0}^{\infty} q^i = \frac{1}{1-q}\\
&\rightsquigarrow \sum_{i=1}^{\infty} (k+1) p^k = \sum_{i=1}^{\infty} i *q^{i-1} = \sum_{i=0}^{\infty} i * q^{i-1} = \frac{1}{(1-q)^2}\\
&\rightsquigarrow \sum_{i=0}^{\infty} (k+1)k * q^{k-1} = \sum_{i=1}^{\infty}\\
(k+1)k *q^{k-1} = \frac{2}{(1-q)^3}\\
&\rightarrow \sum_{i=1}^{\infty} k^2 q^{k-1} + \sum_{i=1}^{\infty} k q^{k-1} =\\
\frac{2}{(1-q)^3}\\
&\rightarrow \sum_{i=1}^{\infty} k^2 * q^{k-1} =\\
\frac{2}{(1-q)^3} - \frac{1}{(1-q)^2} (**)}
\begin{itemize}
  \item \ilmath{E(x) &= \sum_{i=1}^{\infty} i * p_i = \sum_{i=1}^{\infty} i(1-p)^{i-1} p\\
= p \sum_{i=1}^{\infty} i(1-p)^{i-1}\\
      &= p \frac{1}{(1-(1-p))^2} = p * \frac{1}{p^2} = \frac{1}{p}}
    \item \ilmath{D^2(x) &= E(x^2) - (E(x))^2 = E(x^2) - \frac{1}{p^2} = p\\
        \sum_{i=1}^{\infty} i^2 (1-p)^{i-1} - \frac{1}{p^2}\\
      &= p(\frac{2}{p^3} - \ frac{1}{p^3})- \frac{1}{p^2} = \frac{2}{p^2} -\\
      {p}{p^2} - \frac{1}{p^2} = \frac{1-p}{p^2}}
  \end{itemize}
  
  \ssect{Definition}
  Sei X eine diskrete Zufallsgröße \\
  X heist gedächnisslos, wenn gilt:
  \ilmath{\forall x,y: p(x > x+y | X > x) = p(X > y)}
  \ssect{Bemerkung}
  Die bereits verbrachte Wartezeit hat keinen Einfluss auf das Ergebnis
  \ssect{Satz}
  Die geometrische Verteilung ist gedächnisslos.
  \sssect{Beweis}
  \ilmath{p(X > x) &= \sum_{i=x+1}^{\infty} (1-p)^{i-1} p = p((1-p)^x +\\
    (1-p)^{x+1} + (1-p)^{x+2} + \cdots )\\
    &= p(1-p)^x *((1-p)^0 + (1-p)^1 + (1-p)^2 + \cdots)\\
    &= p(1-p)^x * \frac{1}{1-(1-p)} = p(1-p)^x * \frac{1}{p}\\
    &= (1-p)^x}
  Analog gilt:
  \ilmath{p(X > y) = (1-p)^y \text{ und } p(X > y +y) = (1-p)^{x+y}}
  \rule{\textwidth}{0.4pt}

  \ilmath{p(X>x+y | X>x) &= \frac{p(X> x+y \wedge X >x)}{p(X>x)}\\
    &= \frac{p(X>x+y)}{p(X>x)}\\
    &= \frac{(1-p)^{x+y}}{(1-p)^x}\\
    &= (1-p)^y = p(X><) \\
    \hfill\square}
  \sect{Sammelbild Problem}
  $X$ Anzahl der Käufe bis zur Kompletierung der Samlung der Größe n \\
  $X_i$ Anzahl der Käufe in Phase i (d.h. Anzahl der Käufe nach dem (i-1)-ten
  Erfolg bis zum (einschließlich) i-ten Erfolg)\\
  \ilmath{X = \sum_{i=1}^{n} X_i}
  \ssect{Beispiel}
  8 Sammelbilder bilden eine vollständige Sammlung. \\
  Versuchsreihe:
  \ilmath{ \underbrace{1,}_{1} \underbrace{2,}_{2} \underbrace{2,8,}_{3}\\
    \underbrace{1,3,}_{4} \underbrace{3,2,4,}{5} \underbrace{7,}_{6}\\
    \underbrace{8,8,5,}_{7} \underbrace{1,6}_{8}}
  Phasen:\\
  $x_1= 1, x_2 = 1 , x_3 = 2, x_4 = 2, x_5 = 3, x_6=1,x_7=3,x_8=2$
  \begin{itemize}
  \item $X_i$ ist geometrisch verteilt mit $ p_i = \frac{n-(i-1)}{n} \implies E(X_i) =  \frac{1}{p_i} = \frac{n}{n-i+1}$
  \item$ X = \sum_{i=1}^{n} X_i \\
    \implies E(x) = \sum_{i=1}^{n} E(x_i) = \sum_{i=1}^{n} \frac{n}{n-i+1} \\
    = \frac{n}{n+(n-1)+\cdots+ 1} = n \sum_{i=1}^{n} \frac{1}{i}$
    
  \end{itemize}
  \ssect{Abschätzung von $\sum_{i=1}^{n} \frac{1}{i}$}
 \ilmath{\int_1^n \frac{1}{x} dx = \ln {x} |_1^n = \ln{n}- \ln{1} = \ln{n}}
 %% todo img 2 graph
 Genauer:
 \ilmath{\ln{n} < \underbrace{\sum_{i=1}^{n} \frac{1}{i}}{1+ \sum_{i=2}^{n} < 1+ \ln{n}}\\
   \sum_{i=1}^{n} \frac{1}{i} = \ln{n} + o(1)\\
 o(1) \overbrace{\rightarrow}^{n \rightarrow \infty} \gamma \approx 0.5772 (\text{Euler-Konstante})}
  \sect{Negative Binomialverteilung}
  \begin{itemize}
  \item X hat die Wertemenge $\{n,n+1, \cdots\}$
    und die Werteverteilung
    $\begin{pmatrix}
      n & n+1 & \cdots \\
      p_n & p_{n+1} &\cdots
    \end{pmatrix}$
    mit $p(X= i)= p_i = \binom{i-1}{n-1}* (1-p)^{i-n} * p^n$ für $i = n,n+1,
    \cdots$\\
    Insbesonder gilt: $\sum_{i=n}^{\infty} p_i = 1 (*)$
  \item X gibt die Anzahl der Bernoulli-Versuche bis zum n-ten Erfolg an.
    \item $X = X_1 + X_2 + \cdots + X_n$, wobei $X_i$ geometrisch verteilt sit (
      mit Erfolgswarscheinlichkeit p)
    \item $E(X) = n * \frac{1}{p}, D^2(X) = n * \frac{1-p}{p^2} $
      
    \end{itemize}
    \ssect{Beweis zu (*)}
    \ilmath{\binom{i-1}{n-1} &= \binom{i-1}{(i-1)-(n+1)} = \binom{i-1}{i-n}\\
      &= \frac{(i-1)* (i-2) * \cdots (i-(i-n))}{(i-n)!}\\
      &= (-1)^{i-n} \binom{-n}{i-n}}
    \rule{\textwidth}{.4pt}
    \ilmath{\sum_{i=n}^{\infty} & \binom{i-1}{i-n}*(1-p)^{i-n}*p^n\\
      &= p^n * \sum_{i-n = 0}^{\infty} (-1)^{i-n} \binom{-n}{i-n}*(1-p)^{i-n}\\
      &= p^n * \sum_{i-n=0}^{\infty} \binom{-n}{i-n} * (p-1)^{i-n}\\
    &= p^n * (1+(p-1))^{-n} = p^n * p^{-n} = 1}
\end{document}
